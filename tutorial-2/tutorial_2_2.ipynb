{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "83222013",
   "metadata": {},
   "source": [
    "# Learning Distance\n",
    "\n",
    "\n",
    "## Neural Network or Multilayer Perceptron\n",
    "A Multilayer Perceptron consists of an input layer, multiple hiddenlayers and an output layer. The MLP is a feedforward algorithm, because inputs are combined with the initial weights in a weighted sum and subjected to the activation function, just like in the simple Perceptron. But the difference is that each linear combination is propagated to the next layer. Each layer is feeding the next one with the result of their computation. This goes through all the hidden layers to the output layer.\n",
    "\n",
    "The advantage of a multilayer perceptron compared to a classical perceptron (single layer) is that it is capable of learning a function between input and output that is non linear. Therfore it can learn signifacently more complex functions capturing the relation from input to output data.\n",
    "\n",
    "This tutorial will give a brief overview into training and testing an MLP and heighlight typical pitfalls that might occure easily.\n",
    "\n",
    "\n",
    "# Coding: Getting Started\n",
    "Before you get started you need to follow the descriptions of the Git-Repository and make sure that all required packages are properly installed. It is recommended to work in a virtual environment. Start your virtual environment **before** you launch this jupyter-notebook. Then you may need to change the kernel: *Kernel &rarr; Change kernel &rarr; venv*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51364717",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import cv2\n",
    "import os \n",
    "import glob\n",
    "import argparse\n",
    "import data_setup\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import mlp_utils as mlp_utils\n",
    "import train_utils\n",
    "from tqdm.auto import tqdm\n",
    "from model import DistNet_MLP as mlp\n",
    "from matplotlib.pyplot import cm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3dff181",
   "metadata": {},
   "outputs": [],
   "source": [
    "YOLO_MODEL = './model/yolo_model.pt'\n",
    "FILE = '../../data/test/img/2023-6-28_17-06-35-343099.png'\n",
    "\n",
    "PATH_TRAIN = '../../data/train/img'\n",
    "PATH_VAL = '../../data/val/img'\n",
    "PATH_TEST = '../../data/test/img'\n",
    "PATH_CSV_TRAIN = '../../data/train/train.csv'\n",
    "PATH_CSV_VAL = '../../data/val/val.csv'\n",
    "PATH_CSV_TEST = '../../data/test/test.csv'\n",
    "\n",
    "PATH_MODEL_SAVE = './model/mlp_model_trained.pth'\n",
    "ROBOTS = [\"ollie\", \"grace\", \"alan\", \"hermann\", \"kaethe\"]\n",
    "CAM_ROBOT = \"ollie\"\n",
    "MAX_DIST = 5\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "FILE_EXTENSION = \"*.png\"\n",
    "CONFIDENCE = 0.7\n",
    "NUM_EPOCHS = 20\n",
    "RELOAD_WEIGHTS = True\n",
    "LR = 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5babafd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading YOLOv5 Model\n",
    "model_YOLO = torch.hub.load('ultralytics/yolov5', 'custom', path=YOLO_MODEL, verbose=False)\n",
    "model_YOLO.conf = CONFIDENCE\n",
    "model_YOLO.to(DEVICE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14f5bb9f",
   "metadata": {},
   "source": [
    "## Part 1.1: Training a small MLP (multi-layer perceptron)\n",
    "\n",
    "The provided code automatically detects little Lego Robots in an image. For detection we use a fine-tuned YOLOv5 network that estimates bounding boxes, that each capture the rough dimensions of the projected object.\n",
    "Given the height of an object bounding box, the camera's focal length as well as the actual robot's height we compute the absolut distance between \"CAM_ROBOT\" and all visible robots in the image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c7b5435",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the training and validation image sets.\n",
    "file_pattern = os.path.join(PATH_TRAIN, FILE_EXTENSION)\n",
    "train_images = glob.glob(file_pattern)\n",
    "\n",
    "file_pattern = os.path.join(PATH_VAL, FILE_EXTENSION)\n",
    "valid_images = glob.glob(file_pattern)\n",
    "\n",
    "# Pass images through yolo and get bounding boxes with their corresponding csv distance labels\n",
    "train_data = mlp_utils.images2data(model_YOLO, train_images, PATH_CSV_TRAIN, verbose = False)\n",
    "valid_data = mlp_utils.images2data(model_YOLO, valid_images, PATH_CSV_VAL, verbose = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "991c6ea4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the MLP model\n",
    "model_MLP = mlp.MLP()\n",
    "\n",
    "# Define optimizer\n",
    "optimizer = torch.optim.Adam(params=model_MLP.parameters(), lr=LR)\n",
    "loss_fn = torch.nn.MSELoss()\n",
    "\n",
    "# Pass the data through the dataloader\n",
    "train_dataloader, valid_dataloader = data_setup.create_dataloader(train_data, valid_data, batch_size=5)\n",
    "train_features_batch, train_labels_batch = next(iter(train_dataloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45e2d0cd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Set training\n",
    "best_metric = float('inf')  # Initialize with a large value for loss\n",
    "train_loss_plotting = []\n",
    "validation_loss_plotting = []\n",
    "\n",
    "# Training Loop\n",
    "for epoch in tqdm(range(NUM_EPOCHS)):\n",
    "    train_loss = train_utils.train_step(model_MLP, train_dataloader, loss_fn, optimizer, DEVICE)\n",
    "    validation_loss = train_utils.validation_step(model_MLP, valid_dataloader, loss_fn, DEVICE)\n",
    "\n",
    "    # add train loss fro plotting\n",
    "    train_loss_plotting.append(train_loss.item())\n",
    "    validation_loss_plotting.append(validation_loss.item())\n",
    "    \n",
    "    ####################################################\n",
    "    # TODO 1:\n",
    "    # please fill in the code to save \n",
    "    # the currently \"best\" trained model. \n",
    "    # How to decide what the best model is?\n",
    "\n",
    "    torch.save(model_MLP.state_dict(), PATH_MODEL_SAVE)\n",
    "\n",
    "    ####################################################\n",
    "\n",
    "#    if(epoch % 10 == 0):\n",
    "#        print(f\"Epoch: {epoch}\\n------\")\n",
    "#        print(f\"Train loss: {train_loss:.5f}\\n\")\n",
    "        \n",
    "# plotting stuff\n",
    "%matplotlib inline\n",
    "x_err = np.linspace(0, NUM_EPOCHS, NUM_EPOCHS)\n",
    "plt.plot(x_err, train_loss_plotting, label='train loss')\n",
    "plt.plot(x_err, validation_loss_plotting, label='val loss')\n",
    "\n",
    "# Adding labels and title\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('MLP train-loss over epochs')\n",
    "\n",
    "# Display the plot\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c048b9b",
   "metadata": {},
   "source": [
    "### Question: \n",
    "* Can you overfit intentionally to the training data? How does the validation curve will look like? Please verify your assumption.\n",
    "\n",
    "* Now train a well performing model. Feel free to change parameters like learning rate, number of epochs, number of layers. How do these parameters effect the result?\n",
    "\n",
    "*Answer:*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc8f7ac5",
   "metadata": {},
   "source": [
    "## Part 1.2: Inference on a single image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5ee64f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading trained model\n",
    "model = mlp.MLP()\n",
    "model.load_state_dict(torch.load(PATH_MODEL_SAVE, map_location=torch.device(DEVICE)))\n",
    "model.eval() \n",
    "\n",
    "# yolo bounding box detection\n",
    "res = model_YOLO(FILE)\n",
    "\n",
    "bounding_boxes = res.xyxy[0][res.xyxy[0][:, 0].sort()[1]]\n",
    "image = cv2.imread(FILE)\n",
    "color = iter(cm.rainbow(np.linspace(0, 1, 5)))\n",
    "\n",
    "for box in bounding_boxes:\n",
    "    \n",
    "    BBH = (box[3] - box[1]) / 640\n",
    "    \n",
    "    input = torch.stack((torch.tensor(1).to(DEVICE), BBH))\n",
    "    distance = model(input)\n",
    "    \n",
    "    c = next(color)\n",
    "    cv2.rectangle(image, (int(box[0]), int(box[1])), (int(box[2]), int(box[3])), c[0:3]*255, 3)\n",
    "    image_with_rectangle = cv2.rectangle(image, (int(box[0]), int(box[1])-30), (int(box[0]) + 115, int(box[1]-2)), (255,255,255), -1)\n",
    "    img_drawn = cv2.putText(image, f\"Dist: {(distance.item() * MAX_DIST):.3f}m\", (int(box[0]), int(box[1] - 10)), cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0,0,0), 2, cv2.LINE_AA)\n",
    "    image_rgb = cv2.cvtColor(img_drawn, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "\n",
    "# visualize frame with overlayed \n",
    "# bounding boxes and distance estimates\n",
    "%matplotlib inline\n",
    "\n",
    "fig = plt.figure()\n",
    "plt.imshow(image_rgb)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72a51658",
   "metadata": {},
   "source": [
    "## Part 1.3: Visualization\n",
    "\n",
    "... now let's run this code on several consequtive frames and visualize the relationship between object height and its distance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7cbb6b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Loading Pictures...\")\n",
    "\n",
    "file_pattern = os.path.join(PATH_TEST, FILE_EXTENSION)\n",
    "picture_files = glob.glob(file_pattern)\n",
    "picture_files.sort()\n",
    "\n",
    "print(\"Reading pictures...\")\n",
    "\n",
    "values = {}\n",
    "\n",
    "first_image = cv2.imread(picture_files[0])\n",
    "height, width, _ = first_image.shape\n",
    "\n",
    "for i, file in enumerate(picture_files):\n",
    "\n",
    "    res = model_YOLO(file)\n",
    "    bounding_boxes = res.xyxy[0][res.xyxy[0][:, 0].sort()[1]]\n",
    "\n",
    "    for box in bounding_boxes:\n",
    "        BBH = (box[3] - box[1]) / 640\n",
    "\n",
    "        input = torch.stack((torch.tensor(1).to(DEVICE), BBH))\n",
    "        distance = model(input)\n",
    "        \n",
    "        values.setdefault(distance, []).append(box[3] - box[1])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4880940",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "x = []\n",
    "y = []\n",
    "\n",
    "for dist in values:\n",
    "    for height in values[dist]:\n",
    "        x.append(height.item())\n",
    "        y.append(dist.detach().numpy())\n",
    "\n",
    "plt.scatter(x, y)\n",
    "\n",
    "plt.xlabel('Height of the bounding box (in pixel)')\n",
    "plt.ylabel('Distance (in m)')\n",
    "plt.title('Plot of height and distance')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36e75f75",
   "metadata": {},
   "source": [
    "## Part 2.1: Evaluation\n",
    "\n",
    "Groudtruth robot poses were captured with a motion capture system for all robots visible in this data. Poses were saved in the provided .csv file. In this part of the tutorial let's anayse the observed error. How does this relate to the object's distance? What are possible causes of this error?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26447341",
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading trained model\n",
    "model = mlp.MLP()\n",
    "model.load_state_dict(torch.load(PATH_MODEL_SAVE, map_location=torch.device(DEVICE)))\n",
    "model.eval()\n",
    "\n",
    "# get list of picture files in PATH\n",
    "file_pattern = os.path.join(PATH_TEST, FILE_EXTENSION)\n",
    "picture_files = glob.glob(file_pattern)\n",
    "picture_files.sort()\n",
    "\n",
    "# read csv file with ground truth robot poses\n",
    "df = pd.read_csv(PATH_CSV_TEST, header=None)\n",
    "\n",
    "# get column for each robot in csv file\n",
    "row = df.iloc[0].to_numpy()\n",
    "robot_col = {}\n",
    "for rbo in ROBOTS:\n",
    "    bool_array = row == rbo\n",
    "    robot_col[rbo] = bool_array.argmax()\n",
    "cam_robot_col = robot_col[CAM_ROBOT]\n",
    "\n",
    "err = []\n",
    "distance_x = []\n",
    "\n",
    "for i, file in enumerate(picture_files):\n",
    "\n",
    "    # -----------------------------------------------------------------\n",
    "    # LOAD GROUNDTRUTH DISTANCES FOR EACH PICTURE\n",
    "    # -----------------------------------------------------------------\n",
    "    \n",
    "    row_val = os.path.basename(file)\n",
    "    row_matching_value = df[df.iloc[:, 0] == row_val]\n",
    "    print(\"********\")\n",
    "    print(row_val)\n",
    "\n",
    "    if row_matching_value.empty:\n",
    "        print(\"Skipping, since no matching image name in csv\")\n",
    "        continue\n",
    "\n",
    "    row = row_matching_value.index[0]\n",
    "\n",
    "    if type(df.at[row,2]) is not str and math.isnan(df.at[row,2]):\n",
    "        print(\"Skipping, since no robots visable on image \")\n",
    "        continue\n",
    "       \n",
    "    # robots present in current frame\n",
    "    robot_list = df.at[row,2].split()\n",
    "    \n",
    "    distance_gt = []\n",
    "    \n",
    "    for robot in zip(robot_list):\n",
    "        \n",
    "        # pose of the camera robot\n",
    "        camX = df.at[row,cam_robot_col+2]\n",
    "        camY = df.at[row,cam_robot_col]\n",
    "\n",
    "        # pose of neighboring robot\n",
    "        robotX = df.at[row, robot_col[robot[0]] + 2]\n",
    "        robotY = df.at[row, robot_col[robot[0]]]\n",
    "\n",
    "        # GROUNDTRUTH DISTANCE\n",
    "        distance = np.sqrt((float(robotX)-float(camX)) ** 2 + (float(robotY)-float(camY)) ** 2)\n",
    "        distance = distance / 100\n",
    "        \n",
    "        print(f\"distance_gt: {distance}\")\n",
    "        \n",
    "        distance = torch.tensor(distance, dtype = torch.float32,)\n",
    "        distance_gt.append(distance)\n",
    "        \n",
    "    # -----------------------------------------------------------------\n",
    "    # COMPUTE DISTANCE FROM BOUNDING BOX HEIGHT\n",
    "    # -----------------------------------------------------------------\n",
    "    \n",
    "    res = model_YOLO(file)\n",
    "    bounding_boxes = res.xyxy[0][res.xyxy[0][:, 0].sort()[1]]\n",
    "\n",
    "    distance_estim = []\n",
    "    \n",
    "    for box in bounding_boxes:\n",
    "    \n",
    "        BBH = (box[3] - box[1]) / 640\n",
    "\n",
    "        input = torch.stack((torch.tensor(1).to(DEVICE), BBH))\n",
    "        distance = model(input)\n",
    "        distance = distance[0].detach()\n",
    "        distance_estim.append(distance * MAX_DIST)\n",
    "        \n",
    "        print(f\"distance_estim: {distance * MAX_DIST}\")\n",
    "        \n",
    "    # -----------------------------------------------------------------\n",
    "    # COMPUTE ERROR\n",
    "    # -----------------------------------------------------------------\n",
    "    \n",
    "    if(bounding_boxes.shape[0] == len(robot_list)):\n",
    "\n",
    "        error = abs(np.array(distance_gt) - np.array(distance_estim))\n",
    "        err.extend(error)\n",
    "        distance_x.extend(distance_gt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abb6127c",
   "metadata": {},
   "source": [
    "## Part 2.2: Visualization\n",
    "...now let's visualize again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f616b9c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(distance_x, err, label='Error')\n",
    "mean_err = np.mean(err)\n",
    "plt.axhline(y=mean_err, color='r', linestyle='--', label=f'Mean Error: {mean_err:.2f} m')\n",
    "\n",
    "# Adding labels and title\n",
    "plt.xlabel('Distance [m]')\n",
    "plt.ylabel('Error [m]')\n",
    "\n",
    "# Display the plot\n",
    "plt.legend()\n",
    "plt.show(block=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "687b213b",
   "metadata": {},
   "source": [
    "## Question 1:\n",
    "Please describe the behavior of the error. What are possible causes for errors that you observe? (Please name at least three causes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e1ccf9a",
   "metadata": {},
   "source": [
    "*Answer:*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
